{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\36394\\anaconda3\\envs\\art_med\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\36394\\anaconda3\\envs\\art_med\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:25: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in 0.14. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "Using cache found in C:\\Users\\36394/.cache\\torch\\hub\\facebookresearch_pytorchvideo_main\n"
     ]
    }
   ],
   "source": [
    "## Prepare for the pre-trained model\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")\n",
    "from typing import Dict\n",
    "\n",
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cuda\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")\n",
    "\n",
    "\n",
    "####################\n",
    "# SlowFast transform\n",
    "####################\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: breakdancing, yoga, tai chi, dancing ballet, robot dancing\n"
     ]
    }
   ],
   "source": [
    "# Define a function to run the pre-model on a video\n",
    "\n",
    "def run_pretrained_model(video_path):\n",
    "    # Load the example video\n",
    "\n",
    "    # Select the duration of the clip to load by specifying the start and end duration\n",
    "    # The start_sec should correspond to where the action occurs in the video\n",
    "    start_sec = 0\n",
    "    end_sec = start_sec + clip_duration\n",
    "\n",
    "    # Initialize an EncodedVideo helper class\n",
    "    video = EncodedVideo.from_path(video_path)\n",
    "\n",
    "    # Load the desired clip\n",
    "    video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)\n",
    "\n",
    "    # Apply a transform to normalize the video input\n",
    "    video_data = transform(video_data)\n",
    "\n",
    "    # Move the inputs to the desired device\n",
    "    inputs = video_data[\"video\"]\n",
    "    inputs = [i.to(device)[None, ...] for i in inputs]\n",
    "    preds = model(inputs)\n",
    "\n",
    "    # Get the predicted classes\n",
    "    post_act = torch.nn.Softmax(dim=1)\n",
    "    preds = post_act(preds)\n",
    "    pred_classes = preds.topk(k=5).indices\n",
    "\n",
    "    # Map the predicted classes to the label names\n",
    "    pred_class_names = [kinetics_id_to_classname[int(i)] for i in pred_classes[0]]\n",
    "    print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n",
    "\n",
    "# Test the function\n",
    "video_path = \"../path_save_video.mp4\"\n",
    "run_pretrained_model(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Delete the bad data\n",
    "import os\n",
    "# Define the dataset folder\n",
    "dir_10s = r'C:\\Users\\36394\\Study\\GWU\\PHD in Biomedical Engineer\\Research\\FOS\\Autism_dataset\\Data_processed'\n",
    "dir_vision = r'C:\\Users\\36394\\Study\\GWU\\PHD in Biomedical Engineer\\Research\\FOS\\Autism_dataset\\Vision_dataset'\n",
    "dir_train = os.path.join(dir_vision, 'train')\n",
    "dir_val = os.path.join(dir_vision, 'val')\n",
    "\n",
    "g = os.walk(dir_vision)\n",
    "num_bad_video = 0\n",
    "num_video = 0\n",
    "for path,dir_list,file_list in g:\n",
    "    for file_name in file_list:\n",
    "        path_video = os.path.join(path, file_name)\n",
    "        try:\n",
    "            video = EncodedVideo.from_path(path_video)\n",
    "        except:\n",
    "            print('Error: {}'.format(os.path.basename(path_video)))\n",
    "            num_bad_video += 1\n",
    "            os.remove(path_video)\n",
    "        num_video += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: reading book, opening present, cutting nails, hugging, reading newspaper\n",
      "Predicted labels: washing feet, reading book, reading newspaper, cleaning shoes, playing keyboard\n",
      "Error: C+_10.mp4\n",
      "Predicted labels: dunking basketball, passing American football (not in game), playing basketball, throwing ball, drop kicking\n",
      "Predicted labels: blowing out candles, playing keyboard, playing xylophone, opening present, reading book\n",
      "Predicted labels: clean and jerk, deadlifting, snatch weight lifting, bench pressing, squat\n",
      "Predicted labels: somersaulting, drop kicking, breakdancing, faceplanting, tickling\n",
      "Predicted labels: bowling, opening present, waiting in line, auctioning, playing monopoly\n",
      "Predicted labels: blowing out candles, making a sandwich, setting table, making sushi, folding clothes\n",
      "Predicted labels: wrapping present, unboxing, folding paper, reading book, opening present\n",
      "Predicted labels: blowing out candles, setting table, pushing car, headbutting, shaking head\n",
      "Predicted labels: washing hands, hugging, robot dancing, decorating the christmas tree, doing laundry\n",
      "Predicted labels: tai chi, side kick, breakdancing, tango dancing, dancing ballet\n",
      "Predicted labels: playing keyboard, reading book, ironing, folding clothes, playing xylophone\n",
      "Predicted labels: hammer throw, throwing discus, jogging, shaking head, reading newspaper\n",
      "Error: C+_111.mp4\n",
      "Predicted labels: counting money, washing dishes, playing keyboard, playing xylophone, reading book\n",
      "Predicted labels: egg hunting, pushing cart, holding snake, sweeping floor, hugging\n",
      "Predicted labels: giving or receiving award, slapping, hoverboarding, hugging, shaking hands\n",
      "Predicted labels: folding clothes, cleaning shoes, doing laundry, mopping floor, opening present\n",
      "Predicted labels: pushing car, laughing, brushing teeth, crawling baby, pushing cart\n",
      "Predicted labels: unboxing, playing monopoly, counting money, tasting food, reading book\n",
      "Predicted labels: fixing hair, brushing hair, massaging person's head, slapping, cracking neck\n",
      "Error: C+_119.mp4\n",
      "Predicted labels: playing monopoly, reading book, opening present, counting money, ripping paper\n",
      "Predicted labels: fixing hair, holding snake, applying cream, brushing hair, braiding hair\n",
      "Predicted labels: pushing cart, opening present, stretching leg, bending metal, folding clothes\n",
      "Error: C+_122.mp4\n",
      "Predicted labels: washing dishes, doing laundry, baking cookies, brushing hair, fixing hair\n",
      "Predicted labels: cleaning floor, mopping floor, moving furniture, cleaning pool, watering plants\n",
      "Predicted labels: folding paper, counting money, playing chess, writing, rock scissors paper\n",
      "Predicted labels: making jewelry, making sushi, setting table, folding napkins, bookbinding\n",
      "Predicted labels: washing dishes, opening present, washing hands, baking cookies, reading book\n",
      "Predicted labels: folding clothes, doing laundry, opening present, washing dishes, decorating the christmas tree\n",
      "Predicted labels: fixing hair, brushing hair, hugging, hoverboarding, robot dancing\n",
      "Predicted labels: situp, tickling, folding clothes, doing laundry, laughing\n",
      "Predicted labels: sharpening pencil, folding paper, bookbinding, making jewelry, drawing\n",
      "Predicted labels: playing monopoly, counting money, playing chess, ripping paper, making jewelry\n",
      "Predicted labels: playing xylophone, reading book, playing keyboard, folding clothes, making jewelry\n",
      "Predicted labels: playing saxophone, playing didgeridoo, playing bagpipes, waiting in line, decorating the christmas tree\n",
      "Error: C+_134.mp4\n",
      "Predicted labels: yoga, bending back, cleaning windows, deadlifting, clean and jerk\n",
      "Predicted labels: reading book, hugging, cutting nails, carrying baby, tickling\n",
      "Predicted labels: clapping, playing monopoly, rock scissors paper, playing poker, blowing out candles\n",
      "Predicted labels: playing xylophone, setting table, playing keyboard, ironing, folding clothes\n",
      "Predicted labels: opening present, reading book, playing xylophone, egg hunting, playing recorder\n",
      "Error: C+_14.mp4\n",
      "Predicted labels: opening present, fixing hair, reading book, playing monopoly, rock scissors paper\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Define the dataset folder\n",
    "dir_10s = r'C:\\Users\\36394\\Study\\GWU\\PHD in Biomedical Engineer\\Research\\FOS\\Autism_dataset\\Data_processed'\n",
    "dir_vision = r'C:\\Users\\36394\\Study\\GWU\\PHD in Biomedical Engineer\\Research\\FOS\\Autism_dataset\\Vision_dataset'\n",
    "dir_train = os.path.join(dir_vision, 'train')\n",
    "dir_val = os.path.join(dir_vision, 'val')\n",
    "\n",
    "g = os.walk(dir_vision)\n",
    "num_bad_video = 0\n",
    "num_video = 0\n",
    "for path,dir_list,file_list in g:\n",
    "    for file_name in file_list:\n",
    "        path_video = os.path.join(path, file_name)\n",
    "        try:\n",
    "            run_pretrained_model(path_video)\n",
    "        except:\n",
    "            print('Error: {}'.format(os.path.basename(path_video)))\n",
    "            num_bad_video += 1\n",
    "        num_video += 1\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad video: 95\n",
      "Number of video: 3227\n",
      "Percentage of bad video: 0.02943910753021382\n"
     ]
    }
   ],
   "source": [
    "print('Number of bad video: {}'.format(num_bad_video))\n",
    "print('Number of video: {}'.format(num_video))\n",
    "print('Percentage of bad video: {}'.format(num_bad_video/num_video))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
