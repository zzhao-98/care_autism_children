{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:19:20.931250Z",
     "end_time": "2023-04-28T14:19:24.442395Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\36394\\anaconda3\\envs\\art_med\\lib\\site-packages\\torchvision\\transforms\\_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\36394\\anaconda3\\envs\\art_med\\lib\\site-packages\\torchvision\\transforms\\_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\36394\\anaconda3\\envs\\art_med\\lib\\site-packages\\torchvision\\transforms\\functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, cv2, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from FOS_dataset import FOS_set, transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Read processed dataset csv file\n",
    "df_10s = pd.read_csv(r'C:\\Users\\36394\\PycharmProjects\\care_autism_children\\processed_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:19:26.418071Z",
     "end_time": "2023-04-28T14:19:26.442070Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good videos: 8003\n",
      "Number of bad videos: 37\n"
     ]
    }
   ],
   "source": [
    "# Check how many videos can't be opened\n",
    "# Initialize the df_vision dataset\n",
    "bad_videos = []\n",
    "good_videos = []\n",
    "df_vision = pd.DataFrame(columns=['subject_name', 'video_path', 'labels'])\n",
    "for index, row in df_10s.iterrows():\n",
    "    video_path = row['path']\n",
    "    try:\n",
    "        video = EncodedVideo.from_path(video_path)\n",
    "        labels = row['labels'][1:-1].replace(\"'\", \"\").split(', ')\n",
    "        subject_name = video_path.split('Data_processed')[1]\n",
    "        df_vision.loc[len(df_vision.index)] = [subject_name, video_path, labels]\n",
    "        good_videos.append(video_path)\n",
    "    except:\n",
    "        path_bad_video = video_path.split('Data_processed')[1]\n",
    "        # print(\"Bad video: {}, We can't open it!\".format(path_bad_video))\n",
    "        bad_videos.append(path_bad_video)\n",
    "print(\"Number of good videos: {}\".format(len(good_videos)))\n",
    "print(\"Number of bad videos: {}\".format(len(bad_videos)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:19:26.810072Z",
     "end_time": "2023-04-28T14:20:08.176858Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C+: 2224\n",
      "C-: 15\n",
      "PN: 72\n",
      "EA: 3585\n"
     ]
    }
   ],
   "source": [
    "vision_classes = ['C+', 'C-', 'PN', 'EA']\n",
    "df_vision_modified = copy.deepcopy(df_vision)\n",
    "for vision_class in vision_classes:\n",
    "    df_vision_modified[vision_class] = df_vision.apply(lambda row: 1 if vision_class in row['labels'] else 0, axis=1)\n",
    "    print(\"{}: {}\".format(vision_class, np.sum(df_vision_modified[vision_class] == 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:20:08.178858Z",
     "end_time": "2023-04-28T14:20:08.289860Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C+: 2224\n",
      "EA: 3585\n"
     ]
    }
   ],
   "source": [
    "# Drop 'C-' class, too few samples to train\n",
    "vision_classes = ['C+', 'EA']\n",
    "df_vision_modified = copy.deepcopy(df_vision)\n",
    "for vision_class in vision_classes:\n",
    "    df_vision_modified[vision_class] = df_vision.apply(lambda row: 1 if vision_class in row['labels'] else 0, axis=1)\n",
    "    print(\"{}: {}\".format(vision_class, np.sum(df_vision_modified[vision_class] == 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:23:59.098705Z",
     "end_time": "2023-04-28T14:23:59.182700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Split the dataset into train and test by the subject name and do the K-fold cross validation\n",
    "def split_train_test_by_subject(df, test_size=0.2):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    num_subjects = len(df.subject_name.unique())\n",
    "    num_train_subjects = int(num_subjects * (1 - test_size))\n",
    "\n",
    "    names_train_subjects = df.subject_name.unique()[:num_train_subjects]\n",
    "    names_test_subjects = df.subject_name.unique()[num_train_subjects:]\n",
    "\n",
    "    df_train = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    for name in names_train_subjects:\n",
    "        df_train = pd.concat([df_train, df[df['subject_name'] == name]])\n",
    "        df_train = df_train.reset_index(drop=True)\n",
    "    print(\"Number of train samples: {}\".format(len(df_train)))\n",
    "\n",
    "    for name in names_test_subjects:\n",
    "        df_test = pd.concat([df_test, df[df['subject_name'] == name]])\n",
    "        df_test = df_test.reset_index(drop=True)\n",
    "    print(\"Number of test samples: {}\".format(len(df_test)))\n",
    "\n",
    "    return df_train, df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:24:02.640188Z",
     "end_time": "2023-04-28T14:24:02.654186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 6402\n",
      "Number of test samples: 1601\n"
     ]
    }
   ],
   "source": [
    "df_train, df_valid = split_train_test_by_subject(df_vision_modified, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:24:03.030490Z",
     "end_time": "2023-04-28T14:24:09.930018Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the train set\n",
      "C+: 1960\n",
      "EA: 2970\n",
      "This is valid set\n",
      "C+: 264\n",
      "EA: 615\n"
     ]
    }
   ],
   "source": [
    "print('This is the train set')\n",
    "for vision_class in vision_classes:\n",
    "    print(\"{}: {}\".format(vision_class, np.sum(df_train[vision_class] == 1)))\n",
    "print('This is valid set')\n",
    "for vision_class in vision_classes:\n",
    "    print(\"{}: {}\".format(vision_class, np.sum(df_valid[vision_class] == 1)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:24:11.809143Z",
     "end_time": "2023-04-28T14:24:11.821141Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Store the train and val dataset\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_valid.to_csv('val.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T14:24:16.086880Z",
     "end_time": "2023-04-28T14:24:16.138882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "vision_classes = ['C+', 'PN', 'EA']\n",
    "train_set = FOS_set(df_train, list_caring_labels=vision_classes, transform=transform)\n",
    "val_set = FOS_set(df_valid, list_caring_labels=vision_classes, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:01:40.759680Z",
     "end_time": "2023-04-24T15:01:40.774680Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "for a in range(len(df_train)):\n",
    "    b = []\n",
    "    for c in ['C+', 'PN', 'EA']:\n",
    "        b.append(df_train.iloc[a][c])\n",
    "    if not b == train_set[a][1]:\n",
    "        print('Fuck you!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:04:55.946690Z",
     "end_time": "2023-04-24T15:43:13.145555Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "subject_name           \\20th\\BAM\\Hospital_Playtime_New Toys_1.mp4\nvideo_path      C:/Users/36394/Study/GWU/PHD in Biomedical Eng...\nlabels                                       [C+, VI+, Q+, S+, O]\nC+                                                              1\nPN                                                              0\nEA                                                              0\nName: 1, dtype: object"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T15:02:14.938283Z",
     "end_time": "2023-04-24T15:02:14.948284Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Old code just for reference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Define the dataset folder\n",
    "dir_10s = r'C:\\Users\\36394\\Study\\GWU\\PHD in Biomedical Engineer\\Research\\FOS\\Autism_dataset\\Data_processed'\n",
    "names_vision = ['C+', 'C-', 'PN', 'EA']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad video: \\C+\\12th\\LDK\\2_0.mp4, We can't open it!\n",
      "Bad video: \\C+\\5th\\LJ\\45_6.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_22.MP4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_23.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_24.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_25.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_26.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_27.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_28.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_29.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_30.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_31.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_32.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_33.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_34.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_35.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_36.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\37_37.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_52.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_53.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_54.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_55.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_56.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_57.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_58.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_59.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_60.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_61.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_62.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_63.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_64.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_65.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_66.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_67.mp4, We can't open it!\n",
      "Bad video: \\C+\\6th\\KHD\\41_68.mp4, We can't open it!\n",
      "Bad video: \\C-\\6th\\KHD\\41_10.mp4, We can't open it!\n",
      "Bad video: \\PN\\5th\\LJ\\45_3.mp4, We can't open it!\n",
      "Bad video: \\PN\\5th\\LJ\\45_4.mp4, We can't open it!\n",
      "Bad video: \\PN\\5th\\LJ\\45_5.mp4, We can't open it!\n",
      "Bad video: \\PN\\5th\\LJ\\45_6.mp4, We can't open it!\n",
      "Bad video: \\PN\\5th\\LJ\\45_7.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\37_3.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\37_4.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\37_5.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\37_6.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\41_10.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\41_11.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\41_12.mp4, We can't open it!\n",
      "Bad video: \\PN\\6th\\KHD\\41_13.mp4, We can't open it!\n",
      "Bad video: \\EA\\12th\\LDK\\2_5.mp4, We can't open it!\n",
      "Bad video: \\EA\\12th\\LDK\\4_36.mp4, We can't open it!\n",
      "Bad video: \\EA\\15th\\KEJ\\3_13.mp4, We can't open it!\n",
      "Bad video: \\EA\\15th\\KEJ\\3_14.mp4, We can't open it!\n",
      "Bad video: \\EA\\15th\\KEJ\\3_15.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_19.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_20.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_21.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_22.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_23.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_24.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_25.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_26.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_27.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_28.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_29.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_30.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_31.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_32.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_33.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_34.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_35.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_36.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_37.mp4, We can't open it!\n",
      "Bad video: \\EA\\5th\\LJ\\45_38.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\37_0.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_10.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_11.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_12.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_13.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_14.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_15.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_16.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_17.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_18.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_19.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_2.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_20.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_21.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_3.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_4.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_5.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_6.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_7.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_8.mp4, We can't open it!\n",
      "Bad video: \\EA\\6th\\KHD\\43_9.mp4, We can't open it!\n"
     ]
    }
   ],
   "source": [
    "df_vision = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "bad_videos = []\n",
    "for name in names_vision:\n",
    "    g = os.walk(os.path.join(dir_10s, name))\n",
    "\n",
    "    for path,dir_list,file_list in g:\n",
    "        for file_name in file_list:\n",
    "            video_path = os.path.join(path, file_name)\n",
    "            try:\n",
    "                video = EncodedVideo.from_path(video_path)\n",
    "                subject_name = path.split('Data_processed')[1]\n",
    "                label = name\n",
    "                df_vision.loc[len(df_vision.index)] = [subject_name, video_path, label]\n",
    "            except:\n",
    "                path_bad_video = video_path.split('Data_processed')[1]\n",
    "                print(\"Bad video: {}, We can't open it!\".format(path_bad_video))\n",
    "                bad_videos.append(path_bad_video)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95, 3132)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bad_videos), len(df_vision)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C+', 'C-', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'PN', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA', 'EA']\n"
     ]
    }
   ],
   "source": [
    "print([i.split('\\\\')[1] for i in bad_videos])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the distribution of the dataset\n",
    "print(df_vision.label.value_counts())\n",
    "\n",
    "# Split the dataset based on the label\n",
    "df_C_plus = df_vision[df_vision['label'] == 'C+']\n",
    "df_C_minus = df_vision[df_vision['label'] == 'C-']\n",
    "df_PN = df_vision[df_vision['label'] == 'PN']\n",
    "df_EA = df_vision[df_vision['label'] == 'EA']\n",
    "\n",
    "# Sort the label dataset by subject name\n",
    "df_C_plus = df_C_plus.sort_values(by=['subject_name'])\n",
    "df_C_minus = df_C_minus.sort_values(by=['subject_name'])\n",
    "df_PN = df_PN.sort_values(by=['subject_name'])\n",
    "df_EA = df_EA.sort_values(by=['subject_name'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Split the dataset into train and test by the subject name and do the K-fold cross validation\n",
    "def split_train_test_by_subject(df, test_size=0.2):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    # Get the unique subject name\n",
    "    subject_names = df.subject_name.unique()\n",
    "\n",
    "    # Split the subject name into 5 folds\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    kf.get_n_splits(subject_names)\n",
    "\n",
    "    # Split the subject name into train and test\n",
    "    train_subject_names = []\n",
    "    test_subject_names = []\n",
    "    for train_index, test_index in kf.split(subject_names):\n",
    "        train_subject_names.append(subject_names[train_index])\n",
    "        test_subject_names.append(subject_names[test_index])\n",
    "\n",
    "    # Split the dataset into train and test\n",
    "    df_train = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    df_test = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    for i in range(len(train_subject_names)):\n",
    "        df_train = df_train.append(df[df['subject_name'].isin(train_subject_names[i])])\n",
    "        df_test = df_test.append(df[df['subject_name'].isin(test_subject_names[i])])\n",
    "\n",
    "    return df_train, df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the dataset into train and test by the subject name and do the K-fold cross validation\n",
    "def split_train_test_by_subject(df, test_size=0.2):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    num_subjects = len(df.subject_name.unique())\n",
    "    num_train_subjects = int(num_subjects * (1 - test_size))\n",
    "\n",
    "    names_train_subjects = df.subject_name.unique()[:num_train_subjects]\n",
    "    names_test_subjects = df.subject_name.unique()[num_train_subjects:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    df_test = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "\n",
    "    for name in names_train_subjects:\n",
    "        df_train = pd.concat([df_train, df[df['subject_name'] == name]])\n",
    "    print(\"Number of train samples: {}\".format(len(df_train)))\n",
    "\n",
    "    for name in names_test_subjects:\n",
    "        df_test = pd.concat([df_test, df[df['subject_name'] == name]])\n",
    "    print(\"Number of test samples: {}\".format(len(df_test)))\n",
    "\n",
    "    return df_train, df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Handle the data imbalance problem\n",
    "# Random drop some samples for EA class\n",
    "remove_n = 1461\n",
    "drop_indices = np.random.choice(df_EA.index, remove_n, replace=False)\n",
    "df_EA_modified = df_EA.drop(drop_indices)\n",
    "print('For EA class, we have {} samples'.format(len(df_EA_modified)))\n",
    "df_EA_train, df_EA_test = split_train_test_by_subject(df_EA_modified, test_size=0.2)\n",
    "\n",
    "# Keep the C+ class as it is\n",
    "df_C_plus_modified = df_C_plus\n",
    "print('For C+ class, we have {} samples'.format(len(df_C_plus_modified)))\n",
    "df_C_plus_train, df_C_plus_test = split_train_test_by_subject(df_C_plus_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "# Oversample the C- class for 63 times\n",
    "df_C_minus_modified = pd.concat([df_C_minus]*63, ignore_index=True)\n",
    "print('For C- class, we have {} samples'.format(len(df_C_minus_modified)))\n",
    "df_C_minus_train, df_C_minus_test = split_train_test_by_subject(df_C_minus_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "# Oversample the PN class for 30 times\n",
    "df_PN_modified = pd.concat([df_PN]*30, ignore_index=True)\n",
    "print('For PN class, we have {} samples'.format(len(df_PN_modified)))\n",
    "df_PN_train, df_PN_test = split_train_test_by_subject(df_PN_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "df_vision_modified = pd.concat([df_C_plus_modified, df_C_minus_modified, df_PN_modified, df_EA_modified], ignore_index=True)\n",
    "df_train = pd.concat([df_C_plus_train, df_C_minus_train, df_PN_train, df_EA_train], ignore_index=True)\n",
    "df_test = pd.concat([df_C_plus_test, df_C_minus_test, df_PN_test, df_EA_test], ignore_index=True)\n",
    "print(df_vision_modified.label.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Store the train and val dataset\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('val.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(df_train), len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the dataset into train and val based on the label\n",
    "print(df_train.label.value_counts())\n",
    "print(df_test.label.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "# Split the dataset into train and test by the subject name and do the K-fold cross validation\n",
    "def split_train_test_by_subject(df, test_size=0.2):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    # Get the unique subject name\n",
    "    subject_names = df.subject_name.unique()\n",
    "\n",
    "    # Split the subject name into 5 folds\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    kf.get_n_splits(subject_names)\n",
    "\n",
    "    # Split the subject name into train and test\n",
    "    train_subject_names = []\n",
    "    test_subject_names = []\n",
    "    for train_index, test_index in kf.split(subject_names):\n",
    "        train_subject_names.append(subject_names[train_index])\n",
    "        test_subject_names.append(subject_names[test_index])\n",
    "\n",
    "    # Split the dataset into train and test\n",
    "    df_train = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    df_test = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    for i in range(len(train_subject_names)):\n",
    "        df_train = df_train.append(df[df['subject_name'].isin(train_subject_names[i])])\n",
    "        df_test = df_test.append(df[df['subject_name'].isin(test_subject_names[i])])\n",
    "\n",
    "    return df_train, df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Split the dataset into train and test by the subject name and do the K-fold cross validation\n",
    "def split_train_test_by_subject(df, test_size=0.2):\n",
    "    if test_size > 1 or test_size < 0:\n",
    "        raise ValueError(\"test_size must be between 0 and 1\")\n",
    "\n",
    "    num_subjects = len(df.subject_name.unique())\n",
    "    num_train_subjects = int(num_subjects * (1 - test_size))\n",
    "\n",
    "    names_train_subjects = df.subject_name.unique()[:num_train_subjects]\n",
    "    names_test_subjects = df.subject_name.unique()[num_train_subjects:]\n",
    "\n",
    "    df_train = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "    df_test = pd.DataFrame(columns=['subject_name', 'video_path', 'label'])\n",
    "\n",
    "    for name in names_train_subjects:\n",
    "        df_train = pd.concat([df_train, df[df['subject_name'] == name]])\n",
    "    print(\"Number of train samples: {}\".format(len(df_train)))\n",
    "\n",
    "    for name in names_test_subjects:\n",
    "        df_test = pd.concat([df_test, df[df['subject_name'] == name]])\n",
    "    print(\"Number of test samples: {}\".format(len(df_test)))\n",
    "\n",
    "    return df_train, df_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For EA class, we have 820 samples\n",
      "Number of train samples: 729\n",
      "Number of test samples: 91\n",
      "For C+ class, we have 811 samples\n",
      "Number of train samples: 591\n",
      "Number of test samples: 220\n",
      "For C- class, we have 819 samples\n",
      "Number of train samples: 189\n",
      "Number of test samples: 630\n",
      "For PN class, we have 810 samples\n",
      "Number of train samples: 510\n",
      "Number of test samples: 300\n",
      "EA    820\n",
      "C-    819\n",
      "C+    811\n",
      "PN    810\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle the data imbalance problem\n",
    "# Random drop some samples for EA class\n",
    "remove_n = 1461\n",
    "drop_indices = np.random.choice(df_EA.index, remove_n, replace=False)\n",
    "df_EA_modified = df_EA.drop(drop_indices)\n",
    "print('For EA class, we have {} samples'.format(len(df_EA_modified)))\n",
    "df_EA_train, df_EA_test = split_train_test_by_subject(df_EA_modified, test_size=0.2)\n",
    "\n",
    "# Keep the C+ class as it is\n",
    "df_C_plus_modified = df_C_plus\n",
    "print('For C+ class, we have {} samples'.format(len(df_C_plus_modified)))\n",
    "df_C_plus_train, df_C_plus_test = split_train_test_by_subject(df_C_plus_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "# Oversample the C- class for 63 times\n",
    "df_C_minus_modified = pd.concat([df_C_minus]*63, ignore_index=True)\n",
    "print('For C- class, we have {} samples'.format(len(df_C_minus_modified)))\n",
    "df_C_minus_train, df_C_minus_test = split_train_test_by_subject(df_C_minus_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "# Oversample the PN class for 30 times\n",
    "df_PN_modified = pd.concat([df_PN]*30, ignore_index=True)\n",
    "print('For PN class, we have {} samples'.format(len(df_PN_modified)))\n",
    "df_PN_train, df_PN_test = split_train_test_by_subject(df_PN_modified, test_size=0.2)\n",
    "\n",
    "\n",
    "df_vision_modified = pd.concat([df_C_plus_modified, df_C_minus_modified, df_PN_modified, df_EA_modified], ignore_index=True)\n",
    "df_train = pd.concat([df_C_plus_train, df_C_minus_train, df_PN_train, df_EA_train], ignore_index=True)\n",
    "df_test = pd.concat([df_C_plus_test, df_C_minus_test, df_PN_test, df_EA_test], ignore_index=True)\n",
    "print(df_vision_modified.label.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Store the train and val dataset\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_test.to_csv('val.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2019, 1241)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EA    729\n",
      "C+    591\n",
      "PN    510\n",
      "C-    189\n",
      "Name: label, dtype: int64\n",
      "C-    630\n",
      "PN    300\n",
      "C+    220\n",
      "EA     91\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and val based on the label\n",
    "print(df_train.label.value_counts())\n",
    "print(df_test.label.value_counts())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the dataset into train and val based on the label\n",
    "df_train, df_val = train_test_split(df_vision_modified, test_size=0.2, stratify=df_vision_modified['label'], random_state=42)\n",
    "print(df_train.label.value_counts())\n",
    "print(df_val.label.value_counts())\n",
    "\n",
    "# Store the train and val dataset\n",
    "df_train.to_csv('train.csv', index=False)\n",
    "df_val.to_csv('val.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Old way to split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir_train = os.path.join(dir_vision, 'train')\n",
    "dir_val = os.path.join(dir_vision, 'val')\n",
    "if not os.path.exists(dir_train): os.mkdir(dir_train)\n",
    "if not os.path.exists(dir_val): os.mkdir(dir_val)\n",
    "\n",
    "durations = []\n",
    "for name in names_vision:\n",
    "    g = os.walk(os.path.join(dir_10s, name))\n",
    "    dir_target_train = os.path.join(dir_train, name)\n",
    "    dir_target_val = os.path.join(dir_val, name)\n",
    "    if not os.path.exists(dir_target_train): os.mkdir(dir_target_train)\n",
    "    if not os.path.exists(dir_target_val): os.mkdir(dir_target_val)\n",
    "\n",
    "    paths_video = []\n",
    "    for path,dir_list,file_list in g:\n",
    "        for file_name in file_list:\n",
    "            path_video = os.path.join(path, file_name)\n",
    "            paths_video.append(path_video)\n",
    "    for num, path_video in enumerate(paths_video):\n",
    "        cap = cv2.VideoCapture(path_video)\n",
    "        if cap.isOpened():\n",
    "            rate = cap.get(5)\n",
    "            FrameNumber = cap.get(7)\n",
    "            duration = (FrameNumber/rate)\n",
    "            durations.append(duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dir_train = os.path.join(dir_vision, 'train')\n",
    "dir_val = os.path.join(dir_vision, 'val')\n",
    "if not os.path.exists(dir_train): os.mkdir(dir_train)\n",
    "if not os.path.exists(dir_val): os.mkdir(dir_val)\n",
    "\n",
    "for name in names_vision:\n",
    "    g = os.walk(os.path.join(dir_10s, name))\n",
    "    dir_target_train = os.path.join(dir_train, name)\n",
    "    dir_target_val = os.path.join(dir_val, name)\n",
    "    if not os.path.exists(dir_target_train): os.mkdir(dir_target_train)\n",
    "    if not os.path.exists(dir_target_val): os.mkdir(dir_target_val)\n",
    "\n",
    "    paths_video = []\n",
    "    for path,dir_list,file_list in g:\n",
    "        for file_name in file_list:\n",
    "            path_video = os.path.join(path, file_name)\n",
    "            paths_video.append(path_video)\n",
    "    paths_train, paths_val = train_test_split(paths_video, test_size=0.2)\n",
    "    for num, path_train in enumerate(paths_train):\n",
    "        shutil.copyfile(path_train, os.path.join(dir_target_train, '{}_{}.mp4'.format(name, num)))\n",
    "    for num, path_val in enumerate(paths_val):\n",
    "        shutil.copyfile(path_val, os.path.join(dir_target_val, '{}_{}.mp4'.format(name, num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dir_train = os.path.join(dir_vision, 'train')\n",
    "dir_val = os.path.join(dir_vision, 'val')\n",
    "if not os.path.exists(dir_train): os.mkdir(dir_train)\n",
    "if not os.path.exists(dir_val): os.mkdir(dir_val)\n",
    "\n",
    "for name in names_vision:\n",
    "    g = os.walk(os.path.join(dir_10s, name))\n",
    "    dir_target_train = os.path.join(dir_train, name)\n",
    "    dir_target_val = os.path.join(dir_val, name)\n",
    "    if not os.path.exists(dir_target_train): os.mkdir(dir_target_train)\n",
    "    if not os.path.exists(dir_target_val): os.mkdir(dir_target_val)\n",
    "\n",
    "    paths_video = []\n",
    "    for path,dir_list,file_list in g:\n",
    "        for file_name in file_list:\n",
    "            path_video = os.path.join(path, file_name)\n",
    "            paths_video.append(path_video)\n",
    "    paths_train, paths_val = train_test_split(paths_video, test_size=0.2)\n",
    "    for num, path_train in enumerate(paths_train):\n",
    "        shutil.copyfile(path_train, os.path.join(dir_target_train, '{}_{}.mp4'.format(name, num)))\n",
    "    for num, path_val in enumerate(paths_val):\n",
    "        shutil.copyfile(path_val, os.path.join(dir_target_val, '{}_{}.mp4'.format(name, num)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art_med",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "a3b71286187f7d0bf192c63da714a1cee0027619278f52a0c3725940ac5f59e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
